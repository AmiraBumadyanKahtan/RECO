{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is the official YOLOv6 notebook by MeiTuan, and is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \nFor more information please visit https://github.com/meituan/YOLOv6. Thank you!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Introduction\n\nYOLOv6 is a single-stage object detection framework dedicated to industrial applications, with hardware-friendly efficient design and high performance.\n\nYOLOv6-nano achieves 35.0 mAP on COCO val2017 dataset with 1242 FPS on T4 using TensorRT FP16 for bs32 inference, and YOLOv6-s achieves 43.1 mAP on COCO val2017 dataset with 520 FPS on T4 using TensorRT FP16 for bs32 inference.\n\nYOLOv6 is composed of the following methods:\n\nHardware-friendly Design for Backbone and Neck\nEfficient Decoupled Head with SIoU Loss","metadata":{}},{"cell_type":"markdown","source":"# Setup\nClone repo and install dependencies.","metadata":{}},{"cell_type":"code","source":"!pip install h5py\n!pip install typing-extensions\n!pip install wheel","metadata":{"execution":{"iopub.status.busy":"2023-01-16T16:28:06.627790Z","iopub.execute_input":"2023-01-16T16:28:06.628114Z","iopub.status.idle":"2023-01-16T16:28:36.866694Z","shell.execute_reply.started":"2023-01-16T16:28:06.628040Z","shell.execute_reply":"2023-01-16T16:28:36.865523Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (3.1.0)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from h5py) (1.21.6)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py) (1.5.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (4.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (0.37.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/meituan/YOLOv6\n%cd YOLOv6\n%pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\nFirst, download a pretrained model from the YOLOv6 [release](https://github.com/meituan/YOLOv6/releases).","metadata":{}},{"cell_type":"code","source":"# Download a pretrained model\nimport torch\ntorch.hub.download_url_to_file('https://github.com/meituan/YOLOv6/releases/download/0.1.0/yolov6s.pt', 'yolov6s.pt')","metadata":{"execution":{"iopub.status.busy":"2023-01-16T16:28:47.309455Z","iopub.execute_input":"2023-01-16T16:28:47.309834Z","iopub.status.idle":"2023-01-16T16:28:52.284506Z","shell.execute_reply.started":"2023-01-16T16:28:47.309800Z","shell.execute_reply":"2023-01-16T16:28:52.283416Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/36.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52d50379bbd946b2a68dddff41f1d50b"}},"metadata":{}}]},{"cell_type":"markdown","source":"Second, run inference with `tools/infer.py`, and saving results to `runs/inference`. Example inference sources are:\n\n```shell\npython tools/infer.py --weights yolov6s.pt --source img.jpg / imgdir\n                                yolov6n.pt\n```","metadata":{}},{"cell_type":"code","source":"!python tools/infer.py --weights yolov6s.pt --source data/images/image1.jpg\n# show image\nfrom IPython.display import Image\nImage(filename=\"data/images/image2.jpg\", width=600)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate\nValidate a model's accuracy on [COCO](https://cocodataset.org/#home) val or test-dev datasets. Models are downloaded automatically from the [latest YOLOv6 release](https://github.com/meituan/YOLOv6/releases). ","metadata":{}},{"cell_type":"markdown","source":"## COCO val\nDownload COCO val 2017 dataset (1GB - 5000 images), and test model accuracy.","metadata":{}},{"cell_type":"code","source":"# Download COCO val\nimport torch\ntorch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')\n!unzip -q tmp.zip -d ../ && rm tmp.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run yolov6x on coco val\n!python tools/eval.py --weights yolov6s.pt --data data/coco.yaml --img 640","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train coco data\nconf: select config file to specify network/optimizer/hyperparameters\n\ndata: prepare [COCO](http://cocodataset.org)  dataset, [YOLO format coco labes](https://github.com/meituan/YOLOv6/releases/download/0.1.0/coco2017labels.zip) and specify dataset paths in data.yaml\n\nmake sure your dataset structure as fellows:\n```shell\n├── coco\n│   ├── annotations\n│   │   ├── instances_train2017.json\n│   │   └── instances_val2017.json\n│   ├── images\n│   │   ├── train2017\n│   │   └── val2017\n│   ├── labels\n│   │   ├── train2017\n│   │   ├── val2017\n│   ├── LICENSE\n│   ├── README.txt\n```","metadata":{}},{"cell_type":"markdown","source":"## COCO datasets","metadata":{}},{"cell_type":"code","source":"# Download coco datasets and need about 30mins.\n%cd ..\n%cd coco/images\n!wget http://images.cocodataset.org/zips/train2017.zip\n!wget http://images.cocodataset.org/zips/val2017.zip\n!wget http://images.cocodataset.org/zips/test2017.zip\n!unzip train2017.zip && rm train2017.zip\n!unzip val2017.zip && rm val2017.zip\n!unzip test2017.zip && rm test2017.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Before running, you need to make sure you're in the YOLOv6 root directory.\n# Train YOLOv6s on COCO for 30 epochs\n!python tools/train.py --img 640 --batch 32 --epochs 30 --conf configs/yolov6s.py --data data/coco.yaml","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## COCO128 datasets\nYou need create a new file `coco128.yaml` under the folder `./data`.The details are as follows:\n\n```\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../coco128  # dataset root dir\ntrain: images/train2017  # train images (relative to 'path') 128 images\nval: images/train2017  # val images (relative to 'path') 128 images\ntest:  # test images (optional)\n\n# Classes\nnc: 80  # number of classes\nnames: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n        'hair drier', 'toothbrush']  # class names\n```","metadata":{}},{"cell_type":"code","source":"# Download coco128 datasets\ntorch.hub.download_url_to_file('https://ultralytics.com/assets/coco128.zip', 'tmp.zip')\n!unzip -q tmp.zip -d ../ && rm tmp.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train YOLOv6s on COCO128 for 100 epochs\n!python tools/train.py --img 640 --batch 32 --epochs 100 --data data/coco128.yaml","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tensorboard  (optional)\n%load_ext tensorboard\n%tensorboard --logdir runs/train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Custom Data\nThis guidence explains how to train your own custom data with YOLOv6 (take fine-tuning YOLOv6-s model for example).","metadata":{}},{"cell_type":"markdown","source":"## Prepare your own dataset","metadata":{}},{"cell_type":"markdown","source":"**Step 1** Prepare your own dataset with images. For labeling images, you can use tools like [Labelme](https://github.com/wkentaro/labelme).\n\n**Step 2** Generate label files in YOLO format.\n\nOne image corresponds to one label file, and the label format example is presented as below.\n\n```json\n# class_id center_x center_y bbox_width bbox_height\n0 0.300926 0.617063 0.601852 0.765873\n1 0.575 0.319531 0.4 0.551562\n```\n\n- Each row represents one object.\n- Class id starts from `0`.\n- Boundingbox coordinates must be in normalized `xywh` format (from 0 - 1). If your boxes are in pixels, divide `center_x` and `bbox_width` by image width, and `center_y` and `bbox_height` by image height.\n\n**Step 3** Organize directories.\n\nOrganize your directory of custom dataset as follows:\n\n```shell\ncustom_dataset\n├── images\n│   ├── train\n│   │   ├── train0.jpg\n│   │   └── train1.jpg\n│   ├── val\n│   │   ├── val0.jpg\n│   │   └── val1.jpg\n│   └── test\n│       ├── test0.jpg\n│       └── test1.jpg\n└── labels\n    ├── train\n    │   ├── train0.txt\n    │   └── train1.txt\n    ├── val\n    │   ├── val0.txt\n    │   └── val1.txt\n    └── test\n        ├── test0.txt\n        └── test1.txt\n```\n\n**Step 4** Create `dataset.yaml` in `$YOLOv6_DIR/data`.\n\n```yaml\n# Please insure that your custom_dataset are put in same parent dir with YOLOv6_DIR\ntrain: ../custom_dataset/images/train # train images\nval: ../custom_dataset/images/val # val images\ntest: ../custom_dataset/images/test # test images (optional)\n\n# whether it is coco dataset, only coco dataset should be set to True.\nis_coco: False\n\n# Classes\nnc: 20  # number of classes\nnames: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n        'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']  # class names\n```","metadata":{}},{"cell_type":"markdown","source":"## Create a config file","metadata":{}},{"cell_type":"markdown","source":"\nWe use a config file to specify the network structure and training setting, including  optimizer and data augmentation hyperparameters.\n\nIf you create a new config file, please put it under the configs directory.\nOr just use the provided config file in `$YOLOV6_HOME/configs/*_finetune.py`.\n\n```python\n## YOLOv6s Model config file\nmodel = dict(\n    type='YOLOv6s',\n    pretrained='./weights/yolov6s.pt', # download pretrain model from YOLOv6 github if use pretrained model\n    depth_multiple = 0.33,\n    width_multiple = 0.50,\n    ...\n)\nsolver=dict(\n    optim='SGD',\n    lr_scheduler='Cosine',\n    ...\n)\n\ndata_aug = dict(\n    hsv_h=0.015,\n    hsv_s=0.7,\n    hsv_v=0.4,\n    ...\n)\n```","metadata":{}},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"!python tools/ytrain.py --batch 256 --conf configs/yolov6s_finetune.py --data data/data.yaml","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Test Speed","metadata":{}},{"cell_type":"code","source":"!python tools/eval.py --data data/coco128.yaml --batch 32 --weights yolov6s.pt --task speed","metadata":{},"execution_count":null,"outputs":[]}]}